{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9e73933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Question 2 ---\n",
      "How can I filter and view this specific conversation in the Langsmith UI?\n",
      "\n",
      "--- Answer 2 ---\n",
      "I don't know how to filter and view a specific conversation in the Langsmith UI. The provided context only discusses the @traceable decorator and adding metadata to traces, but not filtering or viewing conversations. It does not provide information on using the Langsmith UI for this purpose.\n",
      "\n",
      "\n",
      "Two traces were created with the same 'conversation_id'. Check Langsmith!\n"
     ]
    }
   ],
   "source": [
    "# 4. Second Turn: Continue the Thread\n",
    "\n",
    "question_2 = \"How can I filter and view this specific conversation in the Langsmith UI?\"\n",
    "\n",
    "# Pass the SAME unique_thread_id in langsmith_extra metadata\n",
    "ai_answer_2 = langsmith_conversational_rag(\n",
    "    question=question_2, \n",
    "    langsmith_extra={\"metadata\": {\"conversation_id\": unique_thread_id}}\n",
    ")\n",
    "\n",
    "print(f\"\\n--- Question 2 ---\\n{question_2}\")\n",
    "print(f\"\\n--- Answer 2 ---\\n{ai_answer_2}\")\n",
    "\n",
    "print(\"\\n\\nTwo traces were created with the same 'conversation_id'. Check Langsmith!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daf7600e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Question 1 ---\n",
      "How do I trace my conversation history using Langsmith?\n",
      "\n",
      "--- Answer 1 ---\n",
      "To trace your conversation history, you can use the @traceable decorator on the relevant functions in LangSmith. This decorator will automatically log inputs, outputs, and errors, providing a record of your conversation history. Additionally, you can add custom metadata to the traces using the 'metadata' dictionary or 'langsmith_extra' parameter for more detailed tracking.\n"
     ]
    }
   ],
   "source": [
    "# 3. First Turn: Start the Conversation Thread (TWEAK)\n",
    "\n",
    "question_1 = \"How do I trace my conversation history using Langsmith?\"\n",
    "\n",
    "# TWEAK: Pass the unique_thread_id in langsmith_extra metadata\n",
    "ai_answer_1 = langsmith_conversational_rag(\n",
    "    question=question_1, \n",
    "    langsmith_extra={\"metadata\": {\"conversation_id\": unique_thread_id}}\n",
    ")\n",
    "\n",
    "print(f\"--- Question 1 ---\\n{question_1}\")\n",
    "print(f\"\\n--- Answer 1 ---\\n{ai_answer_1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e64eef4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. RAG Pipeline Functions\n",
    "\n",
    "@traceable(run_type=\"retriever\", name=\"Retrieve Documents\")\n",
    "def retrieve_documents(question: str):\n",
    "    # This invokes the retriever defined in utils.py\n",
    "    return retriever.invoke(question)   \n",
    "\n",
    "@traceable(run_type=\"llm\", name=\"Final LLM Call (Groq)\")\n",
    "def call_llm(messages: List[dict]):\n",
    "    # This LLM call is automatically traced by LangChain/Groq\n",
    "    return llm_client.invoke(messages)\n",
    "\n",
    "@traceable(run_type=\"chain\", name=\"RAG Response Generation\")\n",
    "def generate_response(question: str, documents):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": RAG_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"}\n",
    "    ]\n",
    "    response = call_llm(messages)\n",
    "    return response\n",
    "\n",
    "@traceable(run_type=\"chain\", name=\"Root Conversational RAG\")\n",
    "def langsmith_conversational_rag(question: str, **kwargs):\n",
    "    # kwargs will contain the langsmith_extra metadata\n",
    "    documents = retrieve_documents(question)\n",
    "    response = generate_response(question, documents)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82b4b44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Raghav Gupta\\OneDrive\\Desktop\\Langsmith-MAT496\\utils.py:21: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Unique Thread ID: 9c88833a-c345-4589-a5a1-0e7670169ab9\n"
     ]
    }
   ],
   "source": [
    "# 1. Setup Environment, Imports, and UUID\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langsmith import traceable \n",
    "from langchain_groq import ChatGroq \n",
    "from typing import List\n",
    "import nest_asyncio\n",
    "import uuid\n",
    "from utils import get_vector_db_retriever, RAG_SYSTEM_PROMPT\n",
    "\n",
    "# --- Configuration ---\n",
    "load_dotenv()\n",
    "MODEL_NAME = \"llama-3.3-70b-versatile\" # Using the confirmed, working Groq model\n",
    "llm_client = ChatGroq(model=MODEL_NAME)\n",
    "nest_asyncio.apply()\n",
    "retriever = get_vector_db_retriever()\n",
    "\n",
    "# TWEAK: Generate a unique thread ID for the entire conversation\n",
    "unique_thread_id = str(uuid.uuid4())\n",
    "print(f\"Generated Unique Thread ID: {unique_thread_id}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langsmith_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
