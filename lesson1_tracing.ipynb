{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d84dd370",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import List\n",
    "from langsmith import traceable \n",
    "from langchain_groq import ChatGroq\n",
    "from utils import get_vector_db_retriever\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38e18b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Raghav Gupta\\OneDrive\\Desktop\\Langsmith-MAT496\\utils.py:16: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "# --- Constants and Setup ---\n",
    "MODEL_NAME = \"llama-3.3-70b-versatile\" \n",
    "RAG_SYSTEM_PROMPT = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\"\"\"\n",
    "retriever = get_vector_db_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10775f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Traced Functions (LCEL and Metadata Tweak) ---\n",
    "\n",
    "@traceable(\n",
    "    name=\"Retrieve Documents\", \n",
    "    metadata={\"retriever\": \"FAISS-HuggingFace\"} # YOUR TWEAK: Adding context\n",
    ")\n",
    "def retrieve_documents(question: str):\n",
    "    return retriever.invoke(question)\n",
    "\n",
    "@traceable(\n",
    "    name=\"Generate Response with Groq\", \n",
    "    metadata={\"model_name\": MODEL_NAME} # YOUR TWEAK: Adding context\n",
    ")\n",
    "def generate_response(question: str, documents):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": RAG_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"}\n",
    "    ]\n",
    "    chat = ChatGroq(model=MODEL_NAME)\n",
    "    response = chat.invoke(messages)\n",
    "    return response.content\n",
    "\n",
    "@traceable(name=\"RAG Pipeline\")\n",
    "def langsmith_rag(question: str):\n",
    "    documents = retrieve_documents(question)\n",
    "    answer = generate_response(question, documents)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cc6a819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Question ---\n",
      "How can I trace with the @traceable decorator?\n",
      "\n",
      "--- Answer ---\n",
      "You can trace a function by adding the @traceable decorator to it, which will automatically track its inputs, outputs, and errors in LangSmith. To add metadata, you can pass a 'metadata' dictionary to the @traceable decorator. Additionally, you can also add metadata at runtime using the 'langsmith_extra' parameter.\n"
     ]
    }
   ],
   "source": [
    "# --- Main Execution ---\n",
    "question = \"How can I trace with the @traceable decorator?\"\n",
    "ai_answer = langsmith_rag(question)\n",
    "\n",
    "print(\"--- Question ---\")\n",
    "print(question)\n",
    "print(\"\\n--- Answer ---\")\n",
    "print(ai_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langsmith_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
