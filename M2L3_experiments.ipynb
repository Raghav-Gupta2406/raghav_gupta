{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ded9488a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup Complete. Langsmith and Groq clients initialized.\n",
      "Simple 'correct_label' evaluator defined.\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------\n",
    "# CELL 1: Setup & Simple Custom Evaluator\n",
    "# -----------------------------------------------------\n",
    "import os\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "from langsmith import Client\n",
    "from langchain_groq import ChatGroq \n",
    "from pydantic import BaseModel, Field\n",
    "from langsmith.schemas import Example, Run # Keeping these for cleaner reference in other cells\n",
    "\n",
    "# Suppress the specific LangChain Deprecation Warning (good practice)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"langchain\")\n",
    "\n",
    "# Load environment variables (LANGCHAIN_API_KEY, GROQ_API_KEY, etc.)\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Initialize Groq Client and Langsmith Client\n",
    "# Set low temp for evaluation consistency\n",
    "MODEL_NAME = \"llama-3.3-70b-versatile\"\n",
    "llm_client = ChatGroq(model=MODEL_NAME, temperature=0.0) \n",
    "client = Client()\n",
    "\n",
    "print(\"Setup Complete. Langsmith and Groq clients initialized.\")\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Simple Custom Evaluator\n",
    "# -----------------------------------------------------\n",
    "def correct_label(inputs: dict, reference_outputs: dict, outputs: dict) -> dict:\n",
    "  \"\"\"A very simple evaluator comparing model output to a reference 'output'.\"\"\"\n",
    "  # NOTE: We are comparing the 'output' keys from the provided dicts\n",
    "  score = outputs.get(\"output\") == reference_outputs.get(\"output\") \n",
    "  return {\"score\": int(score), \"key\": \"correct_label\"}\n",
    "\n",
    "print(\"Simple 'correct_label' evaluator defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "580c7216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Application defined using Groq Model: llama-3.3-70b-versatile\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------\n",
    "# CELL 2: RAG Application Definition (Groq-Compatible)\n",
    "# -----------------------------------------------------\n",
    "\n",
    "# --- GLOBAL CONFIGURATION (Will be updated later) ---\n",
    "MODEL_NAME = \"llama-3.3-70b-versatile\" \n",
    "MODEL_PROVIDER = \"groq\"\n",
    "APP_VERSION = 2.0\n",
    "RAG_SYSTEM_PROMPT = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the latest question in the conversation. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\"\"\"\n",
    "# Initial Groq client. This will be updated for the second experiment.\n",
    "llm_client = ChatGroq(model=MODEL_NAME)\n",
    "\n",
    "# --- MOCK Retriever (Simulating the Document Fetch) ---\n",
    "# This simulates the internal RAG component that returns retrieved documents\n",
    "def get_mock_retriever():\n",
    "    return [\n",
    "        Document(page_content=\"LangSmith is a platform for building and evaluating LLM applications.\"),\n",
    "        Document(page_content=\"Experiments in LangSmith allow comparison of different model versions against a single dataset.\"),\n",
    "        Document(page_content=\"The `evaluate` function runs your application and records all traces and metrics.\")\n",
    "    ]\n",
    "\n",
    "# --- RAG Pipeline Functions ---\n",
    "@traceable(run_type=\"chain\")\n",
    "def retrieve_documents(question: str):\n",
    "    return get_mock_retriever()\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def generate_response(question: str, documents):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": RAG_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"}\n",
    "    ]\n",
    "    return call_groq(messages)\n",
    "\n",
    "@traceable(\n",
    "    run_type=\"llm\",\n",
    "    metadata={\n",
    "        \"ls_provider\": MODEL_PROVIDER,\n",
    "        \"ls_model_name\": MODEL_NAME\n",
    "    }\n",
    ")\n",
    "def call_groq(messages: List[dict]):\n",
    "    # Uses the current global llm_client instance (either Llama or Mixtral)\n",
    "    response = llm_client.invoke(messages)\n",
    "    return response\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def langsmith_rag(question: str):\n",
    "    documents = retrieve_documents(question)\n",
    "    response = generate_response(question, documents)\n",
    "    # The evaluation function expects a string output\n",
    "    return response.content\n",
    "\n",
    "print(f\"RAG Application defined using Groq Model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d00b2c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically selected dataset: M2L1 RAG Examples - 15f2633a\n",
      "Custom Evaluators defined.\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------\n",
    "# CELL 3: Experiment Setup (The Core Tweak Cell)\n",
    "# -----------------------------------------------------\n",
    "\n",
    "datasets_list = list(client.list_datasets(limit=1)) \n",
    "\n",
    "# Now use the corrected variable name in the conditional logic\n",
    "if not datasets_list:\n",
    "    # Use a safe fallback name\n",
    "    dataset_name = \"M2L1 RAG Examples - DEFAULT\" \n",
    "    print(\"WARNING: No dynamic dataset found. Using default placeholder name. (Check M2L1 completion)\")\n",
    "else:\n",
    "    # Safely access the first (most recent) dataset in the list\n",
    "    dataset_name = datasets_list[0].name\n",
    "print(f\"Automatically selected dataset: {dataset_name}\")\n",
    "\n",
    "# TWEAK 2 (Impressive Custom Evaluator): Checks adherence to the system prompt\n",
    "def is_three_sentences(reference_outputs: dict, outputs: dict) -> dict:\n",
    "    \"\"\"Evaluator that checks if the model output adheres to the max three-sentence constraint.\"\"\"\n",
    "    answer = outputs[\"output\"].strip()\n",
    "    # Simple sentence count based on common delimiters (. ! ?)\n",
    "    sentence_count = len([s for s in answer.split('.') if s.strip()]) \n",
    "    \n",
    "    # We check if the count is <= 3, enforcing the 'Use three sentences maximum' rule\n",
    "    score = (sentence_count <= 3) \n",
    "    return {\"key\": \"max_three_sentences_check\", \"score\": int(score)}\n",
    "\n",
    "# Original conciseness evaluator\n",
    "def is_concise_enough(reference_outputs: dict, outputs: dict) -> dict:\n",
    "    score = len(outputs[\"output\"]) < 1.5 * len(reference_outputs[\"output\"])\n",
    "    return {\"key\": \"is_concise\", \"score\": int(score)}\n",
    "\n",
    "def target_function(inputs: dict):\n",
    "    \"\"\"Wraps the RAG pipeline to match the `evaluate` function signature.\"\"\"\n",
    "    return langsmith_rag(inputs[\"question\"])\n",
    "\n",
    "print(\"Custom Evaluators defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e969b0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Baseline Experiment: llama-3.3-70b-versatile ---\n",
      "View the evaluation results for experiment: 'Groq-llama-3.3-70b-versatile-V2.0-Baseline-250607f6' at:\n",
      "https://smith.langchain.com/o/6072fe80-253a-475b-81f3-74f20971421c/datasets/30db0c67-cb69-4056-a271-1e22e70fab73/compare?selectedSessions=3ffd6f68-706a-4411-9de1-47d5e24dadea\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:04,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Experiment finished. Check Langsmith.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------\n",
    "# CELL 4: Experiment 1: Groq Llama Baseline\n",
    "# -----------------------------------------------------\n",
    "\n",
    "# Run the first experiment with Groq Llama model (Baseline)\n",
    "print(f\"\\n--- Running Baseline Experiment: {MODEL_NAME} ---\")\n",
    "\n",
    "evaluate(\n",
    "    target_function,\n",
    "    data=dataset_name,\n",
    "    evaluators=[is_concise_enough, is_three_sentences], # Using our custom evaluator\n",
    "    experiment_prefix=f\"Groq-{MODEL_NAME}-V{APP_VERSION}-Baseline\",\n",
    "    num_repetitions=1,\n",
    "    metadata={\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"version\": APP_VERSION,\n",
    "        \"prompt_constraint\": \"max-three-sentences\"\n",
    "    }\n",
    ")\n",
    "print(\"Baseline Experiment finished. Check Langsmith.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dac2a958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Comparison Experiment: mixtral-8x7b-instruct-v0.1 ---\n",
      "View the evaluation results for experiment: 'Groq-mixtral-8x7b-instruct-v0.1-V2.0-Comparison-11741efe' at:\n",
      "https://smith.langchain.com/o/6072fe80-253a-475b-81f3-74f20971421c/datasets/30db0c67-cb69-4056-a271-1e22e70fab73/compare?selectedSessions=47a3f13b-e637-4e62-9603-ed14f3f69616\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Error running target function: Error code: 404 - {'error': {'message': 'The model `mixtral-8x7b-instruct-v0.1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'code': 'model_not_found'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1923, in _forward\n",
      "    fn(*args, langsmith_extra=langsmith_extra)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\2934221684.py\", line 35, in target_function\n",
      "    return langsmith_rag(inputs[\"question\"])\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\887155684.py\", line 55, in langsmith_rag\n",
      "    response = generate_response(question, documents)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\887155684.py\", line 38, in generate_response\n",
      "    return call_groq(messages)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\887155684.py\", line 49, in call_groq\n",
      "    response = llm_client.invoke(messages)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 395, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1023, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 840, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1089, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_groq\\chat_models.py\", line 533, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\groq\\resources\\chat\\completions.py\", line 456, in create\n",
      "    return self._post(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\groq\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\groq\\_base_client.py\", line 1044, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "groq.NotFoundError: Error code: 404 - {'error': {'message': 'The model `mixtral-8x7b-instruct-v0.1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'code': 'model_not_found'}}\n",
      "Error running evaluator <DynamicRunEvaluator is_concise_enough> on run 87170548-1ee4-47e5-9355-bd413e478ba3: TypeError(\"object of type 'NoneType' has no len()\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1619, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\2934221684.py\", line 30, in is_concise_enough\n",
      "    score = len(outputs[\"output\"]) < 1.5 * len(reference_outputs[\"output\"])\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "Error running evaluator <DynamicRunEvaluator is_three_sentences> on run 87170548-1ee4-47e5-9355-bd413e478ba3: AttributeError(\"'NoneType' object has no attribute 'strip'\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1619, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\2934221684.py\", line 20, in is_three_sentences\n",
      "    answer = outputs[\"output\"].strip()\n",
      "AttributeError: 'NoneType' object has no attribute 'strip'\n",
      "1it [00:00,  2.45it/s]Error running target function: Error code: 404 - {'error': {'message': 'The model `mixtral-8x7b-instruct-v0.1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'code': 'model_not_found'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1923, in _forward\n",
      "    fn(*args, langsmith_extra=langsmith_extra)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\2934221684.py\", line 35, in target_function\n",
      "    return langsmith_rag(inputs[\"question\"])\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\887155684.py\", line 55, in langsmith_rag\n",
      "    response = generate_response(question, documents)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\887155684.py\", line 38, in generate_response\n",
      "    return call_groq(messages)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\887155684.py\", line 49, in call_groq\n",
      "    response = llm_client.invoke(messages)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 395, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1023, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 840, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1089, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_groq\\chat_models.py\", line 533, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\groq\\resources\\chat\\completions.py\", line 456, in create\n",
      "    return self._post(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\groq\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\groq\\_base_client.py\", line 1044, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "groq.NotFoundError: Error code: 404 - {'error': {'message': 'The model `mixtral-8x7b-instruct-v0.1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'code': 'model_not_found'}}\n",
      "Error running evaluator <DynamicRunEvaluator is_concise_enough> on run ef185d6d-7c4e-4a12-bb2a-222255a24214: TypeError(\"object of type 'NoneType' has no len()\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1619, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\2934221684.py\", line 30, in is_concise_enough\n",
      "    score = len(outputs[\"output\"]) < 1.5 * len(reference_outputs[\"output\"])\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "Error running evaluator <DynamicRunEvaluator is_three_sentences> on run ef185d6d-7c4e-4a12-bb2a-222255a24214: AttributeError(\"'NoneType' object has no attribute 'strip'\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1619, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\2934221684.py\", line 20, in is_three_sentences\n",
      "    answer = outputs[\"output\"].strip()\n",
      "AttributeError: 'NoneType' object has no attribute 'strip'\n",
      "Error running target function: Error code: 404 - {'error': {'message': 'The model `mixtral-8x7b-instruct-v0.1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'code': 'model_not_found'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1923, in _forward\n",
      "    fn(*args, langsmith_extra=langsmith_extra)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\2934221684.py\", line 35, in target_function\n",
      "    return langsmith_rag(inputs[\"question\"])\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\887155684.py\", line 55, in langsmith_rag\n",
      "    response = generate_response(question, documents)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\887155684.py\", line 38, in generate_response\n",
      "    return call_groq(messages)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\887155684.py\", line 49, in call_groq\n",
      "    response = llm_client.invoke(messages)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 395, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1023, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 840, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1089, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_groq\\chat_models.py\", line 533, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\groq\\resources\\chat\\completions.py\", line 456, in create\n",
      "    return self._post(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\groq\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\groq\\_base_client.py\", line 1044, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "groq.NotFoundError: Error code: 404 - {'error': {'message': 'The model `mixtral-8x7b-instruct-v0.1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'code': 'model_not_found'}}\n",
      "Error running evaluator <DynamicRunEvaluator is_concise_enough> on run fe1c7950-e343-426b-aa51-f20be1eae77a: TypeError(\"object of type 'NoneType' has no len()\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1619, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\2934221684.py\", line 30, in is_concise_enough\n",
      "    score = len(outputs[\"output\"]) < 1.5 * len(reference_outputs[\"output\"])\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "Error running evaluator <DynamicRunEvaluator is_three_sentences> on run fe1c7950-e343-426b-aa51-f20be1eae77a: AttributeError(\"'NoneType' object has no attribute 'strip'\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1619, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\2934221684.py\", line 20, in is_three_sentences\n",
      "    answer = outputs[\"output\"].strip()\n",
      "AttributeError: 'NoneType' object has no attribute 'strip'\n",
      "3it [00:00,  5.56it/s]Error running target function: Error code: 404 - {'error': {'message': 'The model `mixtral-8x7b-instruct-v0.1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'code': 'model_not_found'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1923, in _forward\n",
      "    fn(*args, langsmith_extra=langsmith_extra)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\2934221684.py\", line 35, in target_function\n",
      "    return langsmith_rag(inputs[\"question\"])\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\887155684.py\", line 55, in langsmith_rag\n",
      "    response = generate_response(question, documents)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\887155684.py\", line 38, in generate_response\n",
      "    return call_groq(messages)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\887155684.py\", line 49, in call_groq\n",
      "    response = llm_client.invoke(messages)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 395, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1023, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 840, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1089, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_groq\\chat_models.py\", line 533, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\groq\\resources\\chat\\completions.py\", line 456, in create\n",
      "    return self._post(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\groq\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\groq\\_base_client.py\", line 1044, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "groq.NotFoundError: Error code: 404 - {'error': {'message': 'The model `mixtral-8x7b-instruct-v0.1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'code': 'model_not_found'}}\n",
      "Error running evaluator <DynamicRunEvaluator is_concise_enough> on run 67e78e17-68ba-445f-95dc-cddc29791883: TypeError(\"object of type 'NoneType' has no len()\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1619, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\2934221684.py\", line 30, in is_concise_enough\n",
      "    score = len(outputs[\"output\"]) < 1.5 * len(reference_outputs[\"output\"])\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "Error running evaluator <DynamicRunEvaluator is_three_sentences> on run 67e78e17-68ba-445f-95dc-cddc29791883: AttributeError(\"'NoneType' object has no attribute 'strip'\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1619, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\2934221684.py\", line 20, in is_three_sentences\n",
      "    answer = outputs[\"output\"].strip()\n",
      "AttributeError: 'NoneType' object has no attribute 'strip'\n",
      "Error running target function: Error code: 404 - {'error': {'message': 'The model `mixtral-8x7b-instruct-v0.1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'code': 'model_not_found'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1923, in _forward\n",
      "    fn(*args, langsmith_extra=langsmith_extra)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\2934221684.py\", line 35, in target_function\n",
      "    return langsmith_rag(inputs[\"question\"])\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\887155684.py\", line 55, in langsmith_rag\n",
      "    response = generate_response(question, documents)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\887155684.py\", line 38, in generate_response\n",
      "    return call_groq(messages)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\887155684.py\", line 49, in call_groq\n",
      "    response = llm_client.invoke(messages)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 395, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1023, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 840, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1089, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_groq\\chat_models.py\", line 533, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\groq\\resources\\chat\\completions.py\", line 456, in create\n",
      "    return self._post(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\groq\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\groq\\_base_client.py\", line 1044, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "groq.NotFoundError: Error code: 404 - {'error': {'message': 'The model `mixtral-8x7b-instruct-v0.1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'code': 'model_not_found'}}\n",
      "Error running evaluator <DynamicRunEvaluator is_concise_enough> on run 280024e8-31a7-471f-9866-0f333643d4fb: TypeError(\"object of type 'NoneType' has no len()\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1619, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\2934221684.py\", line 30, in is_concise_enough\n",
      "    score = len(outputs[\"output\"]) < 1.5 * len(reference_outputs[\"output\"])\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "Error running evaluator <DynamicRunEvaluator is_three_sentences> on run 280024e8-31a7-471f-9866-0f333643d4fb: AttributeError(\"'NoneType' object has no attribute 'strip'\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1619, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\2934221684.py\", line 20, in is_three_sentences\n",
      "    answer = outputs[\"output\"].strip()\n",
      "AttributeError: 'NoneType' object has no attribute 'strip'\n",
      "5it [00:00,  6.97it/s]Error running target function: Error code: 404 - {'error': {'message': 'The model `mixtral-8x7b-instruct-v0.1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'code': 'model_not_found'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1923, in _forward\n",
      "    fn(*args, langsmith_extra=langsmith_extra)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\2934221684.py\", line 35, in target_function\n",
      "    return langsmith_rag(inputs[\"question\"])\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\887155684.py\", line 55, in langsmith_rag\n",
      "    response = generate_response(question, documents)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\887155684.py\", line 38, in generate_response\n",
      "    return call_groq(messages)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\887155684.py\", line 49, in call_groq\n",
      "    response = llm_client.invoke(messages)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 395, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1023, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 840, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1089, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_groq\\chat_models.py\", line 533, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\groq\\resources\\chat\\completions.py\", line 456, in create\n",
      "    return self._post(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\groq\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\groq\\_base_client.py\", line 1044, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "groq.NotFoundError: Error code: 404 - {'error': {'message': 'The model `mixtral-8x7b-instruct-v0.1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'code': 'model_not_found'}}\n",
      "Error running evaluator <DynamicRunEvaluator is_concise_enough> on run 5abe8778-59f8-438a-84d7-7c007ebbc6e0: TypeError(\"object of type 'NoneType' has no len()\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1619, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\2934221684.py\", line 30, in is_concise_enough\n",
      "    score = len(outputs[\"output\"]) < 1.5 * len(reference_outputs[\"output\"])\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "Error running evaluator <DynamicRunEvaluator is_three_sentences> on run 5abe8778-59f8-438a-84d7-7c007ebbc6e0: AttributeError(\"'NoneType' object has no attribute 'strip'\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1619, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\2934221684.py\", line 20, in is_three_sentences\n",
      "    answer = outputs[\"output\"].strip()\n",
      "AttributeError: 'NoneType' object has no attribute 'strip'\n",
      "6it [00:00,  7.21it/s]Error running target function: Error code: 404 - {'error': {'message': 'The model `mixtral-8x7b-instruct-v0.1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'code': 'model_not_found'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1923, in _forward\n",
      "    fn(*args, langsmith_extra=langsmith_extra)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\2934221684.py\", line 35, in target_function\n",
      "    return langsmith_rag(inputs[\"question\"])\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\887155684.py\", line 55, in langsmith_rag\n",
      "    response = generate_response(question, documents)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\887155684.py\", line 38, in generate_response\n",
      "    return call_groq(messages)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\887155684.py\", line 49, in call_groq\n",
      "    response = llm_client.invoke(messages)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 395, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1023, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 840, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1089, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_groq\\chat_models.py\", line 533, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\groq\\resources\\chat\\completions.py\", line 456, in create\n",
      "    return self._post(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\groq\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\groq\\_base_client.py\", line 1044, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "groq.NotFoundError: Error code: 404 - {'error': {'message': 'The model `mixtral-8x7b-instruct-v0.1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'code': 'model_not_found'}}\n",
      "Error running evaluator <DynamicRunEvaluator is_concise_enough> on run 0d7c8e48-d156-4ce8-b9cb-a87d93e71e92: TypeError(\"object of type 'NoneType' has no len()\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1619, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\2934221684.py\", line 30, in is_concise_enough\n",
      "    score = len(outputs[\"output\"]) < 1.5 * len(reference_outputs[\"output\"])\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "Error running evaluator <DynamicRunEvaluator is_three_sentences> on run 0d7c8e48-d156-4ce8-b9cb-a87d93e71e92: AttributeError(\"'NoneType' object has no attribute 'strip'\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1619, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\2934221684.py\", line 20, in is_three_sentences\n",
      "    answer = outputs[\"output\"].strip()\n",
      "AttributeError: 'NoneType' object has no attribute 'strip'\n",
      "7it [00:01,  7.45it/s]Error running target function: Error code: 404 - {'error': {'message': 'The model `mixtral-8x7b-instruct-v0.1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'code': 'model_not_found'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1923, in _forward\n",
      "    fn(*args, langsmith_extra=langsmith_extra)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\2934221684.py\", line 35, in target_function\n",
      "    return langsmith_rag(inputs[\"question\"])\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\887155684.py\", line 55, in langsmith_rag\n",
      "    response = generate_response(question, documents)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\887155684.py\", line 38, in generate_response\n",
      "    return call_groq(messages)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\887155684.py\", line 49, in call_groq\n",
      "    response = llm_client.invoke(messages)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 395, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1023, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 840, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1089, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_groq\\chat_models.py\", line 533, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\groq\\resources\\chat\\completions.py\", line 456, in create\n",
      "    return self._post(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\groq\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\groq\\_base_client.py\", line 1044, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "groq.NotFoundError: Error code: 404 - {'error': {'message': 'The model `mixtral-8x7b-instruct-v0.1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'code': 'model_not_found'}}\n",
      "Error running evaluator <DynamicRunEvaluator is_concise_enough> on run 9d11484c-c92d-48b7-87e0-40f613824aa8: TypeError(\"object of type 'NoneType' has no len()\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1619, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\2934221684.py\", line 30, in is_concise_enough\n",
      "    score = len(outputs[\"output\"]) < 1.5 * len(reference_outputs[\"output\"])\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "Error running evaluator <DynamicRunEvaluator is_three_sentences> on run 9d11484c-c92d-48b7-87e0-40f613824aa8: AttributeError(\"'NoneType' object has no attribute 'strip'\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1619, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\2934221684.py\", line 20, in is_three_sentences\n",
      "    answer = outputs[\"output\"].strip()\n",
      "AttributeError: 'NoneType' object has no attribute 'strip'\n",
      "8it [00:01,  7.62it/s]Error running target function: Error code: 404 - {'error': {'message': 'The model `mixtral-8x7b-instruct-v0.1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'code': 'model_not_found'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1923, in _forward\n",
      "    fn(*args, langsmith_extra=langsmith_extra)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\2934221684.py\", line 35, in target_function\n",
      "    return langsmith_rag(inputs[\"question\"])\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\887155684.py\", line 55, in langsmith_rag\n",
      "    response = generate_response(question, documents)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\887155684.py\", line 38, in generate_response\n",
      "    return call_groq(messages)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\887155684.py\", line 49, in call_groq\n",
      "    response = llm_client.invoke(messages)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 395, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1023, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 840, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1089, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_groq\\chat_models.py\", line 533, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\groq\\resources\\chat\\completions.py\", line 456, in create\n",
      "    return self._post(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\groq\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\groq\\_base_client.py\", line 1044, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "groq.NotFoundError: Error code: 404 - {'error': {'message': 'The model `mixtral-8x7b-instruct-v0.1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'code': 'model_not_found'}}\n",
      "Error running evaluator <DynamicRunEvaluator is_concise_enough> on run 16458570-4e80-42b3-b9f9-19bf2995c029: TypeError(\"object of type 'NoneType' has no len()\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1619, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\2934221684.py\", line 30, in is_concise_enough\n",
      "    score = len(outputs[\"output\"]) < 1.5 * len(reference_outputs[\"output\"])\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "Error running evaluator <DynamicRunEvaluator is_three_sentences> on run 16458570-4e80-42b3-b9f9-19bf2995c029: AttributeError(\"'NoneType' object has no attribute 'strip'\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1619, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\2934221684.py\", line 20, in is_three_sentences\n",
      "    answer = outputs[\"output\"].strip()\n",
      "AttributeError: 'NoneType' object has no attribute 'strip'\n",
      "9it [00:01,  7.73it/s]Error running target function: Error code: 404 - {'error': {'message': 'The model `mixtral-8x7b-instruct-v0.1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'code': 'model_not_found'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1923, in _forward\n",
      "    fn(*args, langsmith_extra=langsmith_extra)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\2934221684.py\", line 35, in target_function\n",
      "    return langsmith_rag(inputs[\"question\"])\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\887155684.py\", line 55, in langsmith_rag\n",
      "    response = generate_response(question, documents)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\887155684.py\", line 38, in generate_response\n",
      "    return call_groq(messages)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\887155684.py\", line 49, in call_groq\n",
      "    response = llm_client.invoke(messages)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 395, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1023, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 840, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1089, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_groq\\chat_models.py\", line 533, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\groq\\resources\\chat\\completions.py\", line 456, in create\n",
      "    return self._post(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\groq\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\groq\\_base_client.py\", line 1044, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "groq.NotFoundError: Error code: 404 - {'error': {'message': 'The model `mixtral-8x7b-instruct-v0.1` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'code': 'model_not_found'}}\n",
      "Error running evaluator <DynamicRunEvaluator is_concise_enough> on run 5efd332c-5258-4e9f-b331-047b8e84171a: TypeError(\"object of type 'NoneType' has no len()\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1619, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\2934221684.py\", line 30, in is_concise_enough\n",
      "    score = len(outputs[\"output\"]) < 1.5 * len(reference_outputs[\"output\"])\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "Error running evaluator <DynamicRunEvaluator is_three_sentences> on run 5efd332c-5258-4e9f-b331-047b8e84171a: AttributeError(\"'NoneType' object has no attribute 'strip'\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1619, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\2934221684.py\", line 20, in is_three_sentences\n",
      "    answer = outputs[\"output\"].strip()\n",
      "AttributeError: 'NoneType' object has no attribute 'strip'\n",
      "10it [00:02,  4.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison Experiment finished. Check Langsmith.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------\n",
    "# CELL 5: Experiment 2: Groq Mixtral Comparison\n",
    "# -----------------------------------------------------\n",
    "\n",
    "# Change the model globally for the next experiment\n",
    "# FIX: Use the correct, full model path for Mixtral on Groq.\n",
    "MODEL_NAME = \"mixtral-8x7b-instruct-v0.1\" \n",
    "llm_client = ChatGroq(model=MODEL_NAME) # New Groq client instance\n",
    "\n",
    "# Run the comparison experiment\n",
    "print(f\"\\n--- Running Comparison Experiment: {MODEL_NAME} ---\")\n",
    "\n",
    "evaluate(\n",
    "    target_function,\n",
    "    data=dataset_name,\n",
    "    evaluators=[is_concise_enough, is_three_sentences],\n",
    "    experiment_prefix=f\"Groq-{MODEL_NAME}-V{APP_VERSION}-Comparison\",\n",
    "    num_repetitions=1,\n",
    "    metadata={\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"version\": APP_VERSION,\n",
    "        \"prompt_constraint\": \"max-three-sentences\"\n",
    "    }\n",
    ")\n",
    "print(\"Comparison Experiment finished. Check Langsmith.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c2135eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Load Test Experiment (Concurrency & Repetitions) ---\n",
      "View the evaluation results for experiment: 'Groq-LoadTest-Reps3-Conc5-e929dfa8' at:\n",
      "https://smith.langchain.com/o/6072fe80-253a-475b-81f3-74f20971421c/datasets/30db0c67-cb69-4056-a271-1e22e70fab73/compare?selectedSessions=d38f8fe8-9ca5-4047-a908-48029da81b66\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:01, 13.99it/s]Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k3603hp4enarfyyw83rhxsrb` service tier `on_demand` on requests per minute (RPM): Limit 30, Used 30, Requested 1. Please try again in 1.967s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'requests', 'code': 'rate_limit_exceeded'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1923, in _forward\n",
      "    fn(*args, langsmith_extra=langsmith_extra)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\2934221684.py\", line 35, in target_function\n",
      "    return langsmith_rag(inputs[\"question\"])\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\887155684.py\", line 55, in langsmith_rag\n",
      "    response = generate_response(question, documents)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\887155684.py\", line 38, in generate_response\n",
      "    return call_groq(messages)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\887155684.py\", line 49, in call_groq\n",
      "    response = llm_client.invoke(messages)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 395, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1023, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 840, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1089, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_groq\\chat_models.py\", line 533, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\groq\\resources\\chat\\completions.py\", line 456, in create\n",
      "    return self._post(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\groq\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\groq\\_base_client.py\", line 1044, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "groq.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k3603hp4enarfyyw83rhxsrb` service tier `on_demand` on requests per minute (RPM): Limit 30, Used 30, Requested 1. Please try again in 1.967s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'requests', 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator is_concise_enough> on run dfcceb4e-c7b0-4558-87e1-ebfaa6035d3f: TypeError(\"object of type 'NoneType' has no len()\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1619, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\2934221684.py\", line 30, in is_concise_enough\n",
      "    score = len(outputs[\"output\"]) < 1.5 * len(reference_outputs[\"output\"])\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "22it [00:05,  1.65it/s]Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k3603hp4enarfyyw83rhxsrb` service tier `on_demand` on requests per minute (RPM): Limit 30, Used 30, Requested 1. Please try again in 1.743s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'requests', 'code': 'rate_limit_exceeded'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1923, in _forward\n",
      "    fn(*args, langsmith_extra=langsmith_extra)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\2934221684.py\", line 35, in target_function\n",
      "    return langsmith_rag(inputs[\"question\"])\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\887155684.py\", line 55, in langsmith_rag\n",
      "    response = generate_response(question, documents)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\887155684.py\", line 38, in generate_response\n",
      "    return call_groq(messages)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\887155684.py\", line 49, in call_groq\n",
      "    response = llm_client.invoke(messages)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 395, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1023, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 840, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1089, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_groq\\chat_models.py\", line 533, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\groq\\resources\\chat\\completions.py\", line 456, in create\n",
      "    return self._post(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\groq\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\groq\\_base_client.py\", line 1044, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "groq.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k3603hp4enarfyyw83rhxsrb` service tier `on_demand` on requests per minute (RPM): Limit 30, Used 30, Requested 1. Please try again in 1.743s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'requests', 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator is_concise_enough> on run 9f8531a1-7cf0-47ca-825b-6f03ca3963e9: TypeError(\"object of type 'NoneType' has no len()\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1619, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\2934221684.py\", line 30, in is_concise_enough\n",
      "    score = len(outputs[\"output\"]) < 1.5 * len(reference_outputs[\"output\"])\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "24it [00:06,  2.14it/s]Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k3603hp4enarfyyw83rhxsrb` service tier `on_demand` on requests per minute (RPM): Limit 30, Used 30, Requested 1. Please try again in 1.27s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'requests', 'code': 'rate_limit_exceeded'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1923, in _forward\n",
      "    fn(*args, langsmith_extra=langsmith_extra)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\2934221684.py\", line 35, in target_function\n",
      "    return langsmith_rag(inputs[\"question\"])\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\887155684.py\", line 55, in langsmith_rag\n",
      "    response = generate_response(question, documents)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\887155684.py\", line 38, in generate_response\n",
      "    return call_groq(messages)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\887155684.py\", line 49, in call_groq\n",
      "    response = llm_client.invoke(messages)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 395, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1023, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 840, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1089, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_groq\\chat_models.py\", line 533, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\groq\\resources\\chat\\completions.py\", line 456, in create\n",
      "    return self._post(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\groq\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\groq\\_base_client.py\", line 1044, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "groq.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k3603hp4enarfyyw83rhxsrb` service tier `on_demand` on requests per minute (RPM): Limit 30, Used 30, Requested 1. Please try again in 1.27s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'requests', 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator is_concise_enough> on run f06c416a-fc0d-4abb-bb11-c5b89689ebf7: TypeError(\"object of type 'NoneType' has no len()\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1619, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\2934221684.py\", line 30, in is_concise_enough\n",
      "    score = len(outputs[\"output\"]) < 1.5 * len(reference_outputs[\"output\"])\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k3603hp4enarfyyw83rhxsrb` service tier `on_demand` on requests per minute (RPM): Limit 30, Used 30, Requested 1. Please try again in 1.876s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'requests', 'code': 'rate_limit_exceeded'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1923, in _forward\n",
      "    fn(*args, langsmith_extra=langsmith_extra)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\2934221684.py\", line 35, in target_function\n",
      "    return langsmith_rag(inputs[\"question\"])\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\887155684.py\", line 55, in langsmith_rag\n",
      "    response = generate_response(question, documents)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\887155684.py\", line 38, in generate_response\n",
      "    return call_groq(messages)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\887155684.py\", line 49, in call_groq\n",
      "    response = llm_client.invoke(messages)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 395, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1023, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 840, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1089, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_groq\\chat_models.py\", line 533, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\groq\\resources\\chat\\completions.py\", line 456, in create\n",
      "    return self._post(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\groq\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\groq\\_base_client.py\", line 1044, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "groq.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k3603hp4enarfyyw83rhxsrb` service tier `on_demand` on requests per minute (RPM): Limit 30, Used 30, Requested 1. Please try again in 1.876s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'requests', 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator is_concise_enough> on run 07d286b0-69df-4105-b245-c1bac31fa7ec: TypeError(\"object of type 'NoneType' has no len()\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1619, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\2934221684.py\", line 30, in is_concise_enough\n",
      "    score = len(outputs[\"output\"]) < 1.5 * len(reference_outputs[\"output\"])\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "27it [00:08,  1.84it/s]Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k3603hp4enarfyyw83rhxsrb` service tier `on_demand` on requests per minute (RPM): Limit 30, Used 30, Requested 1. Please try again in 1.984s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'requests', 'code': 'rate_limit_exceeded'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1923, in _forward\n",
      "    fn(*args, langsmith_extra=langsmith_extra)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\2934221684.py\", line 35, in target_function\n",
      "    return langsmith_rag(inputs[\"question\"])\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\887155684.py\", line 55, in langsmith_rag\n",
      "    response = generate_response(question, documents)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\887155684.py\", line 38, in generate_response\n",
      "    return call_groq(messages)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\887155684.py\", line 49, in call_groq\n",
      "    response = llm_client.invoke(messages)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 395, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1023, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 840, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1089, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_groq\\chat_models.py\", line 533, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\groq\\resources\\chat\\completions.py\", line 456, in create\n",
      "    return self._post(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\groq\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\groq\\_base_client.py\", line 1044, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "groq.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k3603hp4enarfyyw83rhxsrb` service tier `on_demand` on requests per minute (RPM): Limit 30, Used 30, Requested 1. Please try again in 1.984s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'requests', 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator is_concise_enough> on run 1decc594-098b-47f6-b975-8990ccfea38c: TypeError(\"object of type 'NoneType' has no len()\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1619, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\2934221684.py\", line 30, in is_concise_enough\n",
      "    score = len(outputs[\"output\"]) < 1.5 * len(reference_outputs[\"output\"])\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "29it [00:10,  1.43it/s]Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k3603hp4enarfyyw83rhxsrb` service tier `on_demand` on requests per minute (RPM): Limit 30, Used 30, Requested 1. Please try again in 1.468s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'requests', 'code': 'rate_limit_exceeded'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1923, in _forward\n",
      "    fn(*args, langsmith_extra=langsmith_extra)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\2934221684.py\", line 35, in target_function\n",
      "    return langsmith_rag(inputs[\"question\"])\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\887155684.py\", line 55, in langsmith_rag\n",
      "    response = generate_response(question, documents)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\887155684.py\", line 38, in generate_response\n",
      "    return call_groq(messages)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\887155684.py\", line 49, in call_groq\n",
      "    response = llm_client.invoke(messages)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 395, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1023, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 840, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1089, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langchain_groq\\chat_models.py\", line 533, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\groq\\resources\\chat\\completions.py\", line 456, in create\n",
      "    return self._post(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\groq\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\groq\\_base_client.py\", line 1044, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "groq.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k3603hp4enarfyyw83rhxsrb` service tier `on_demand` on requests per minute (RPM): Limit 30, Used 30, Requested 1. Please try again in 1.468s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'requests', 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator is_concise_enough> on run 654b5940-7a28-4f9d-a842-7eea7177ce7e: TypeError(\"object of type 'NoneType' has no len()\")\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1619, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"c:\\Users\\Raghav Gupta\\anaconda3\\envs\\langsmith_env\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Raghav Gupta\\AppData\\Local\\Temp\\ipykernel_46000\\2934221684.py\", line 30, in is_concise_enough\n",
      "    score = len(outputs[\"output\"]) < 1.5 * len(reference_outputs[\"output\"])\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "30it [00:11,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Test Experiment finished. Check Langsmith.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------\n",
    "# CELL 6: Testing Other Parameters (Concurrency & Repetition)\n",
    "# -----------------------------------------------------\n",
    "\n",
    "# Tweak: Combine Concurrency and Repetition into a single powerful test\n",
    "print(\"\\n--- Running Load Test Experiment (Concurrency & Repetitions) ---\")\n",
    "\n",
    "MODEL_NAME = \"llama-3.3-70b-versatile\"\n",
    "llm_client = ChatGroq(model=MODEL_NAME) # Reset model for this test\n",
    "\n",
    "evaluate(\n",
    "    target_function,\n",
    "    data=dataset_name,\n",
    "    evaluators=[is_concise_enough],\n",
    "    experiment_prefix=f\"Groq-LoadTest-Reps3-Conc5\",\n",
    "    num_repetitions=3,\n",
    "    max_concurrency=5, # Run 5 threads concurrently, 3 times each (15 total runs)\n",
    "    metadata={\n",
    "        \"test_type\": \"stress_test\",\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"repetitions\": 3,\n",
    "        \"concurrency\": 5\n",
    "    }\n",
    ")\n",
    "print(\"Load Test Experiment finished. Check Langsmith.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langsmith_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
